@InProceedings{10.1007/978-3-030-45442-5_69,
author="Ionescu, Bogdan
and M{\"u}ller, Henning
and P{\'e}teri, Renaud
and Dang-Nguyen, Duc-Tien
and Zhou, Liting
and Piras, Luca
and Riegler, Michael
and Halvorsen, P{\aa}l
and Tran, Minh-Triet
and Lux, Mathias
and Gurrin, Cathal
and Chamberlain, Jon
and Clark, Adrian
and Campello, Antonio
and Seco de Herrera, Alba G.
and Ben Abacha, Asma
and Datla, Vivek
and Hasan, Sadid A.
and Liu, Joey
and Demner-Fushman, Dina
and Pelka, Obioma
and Friedrich, Christoph M.
and Dicente Cid, Yashin
and Kozlovski, Serge
and Liauchuk, Vitali
and Kovalev, Vassili
and Berari, Raul
and Brie, Paul
and Fichou, Dimitri
and Dogariu, Mihai
and Stefan, Liviu Daniel
and Constantin, Mihai Gabriel",
editor="Jose, Joemon M.
and Yilmaz, Emine
and Magalh{\~a}es, Jo{\~a}o
and Castells, Pablo
and Ferro, Nicola
and Silva, M{\'a}rio J.
and Martins, Fl{\'a}vio",
title="ImageCLEF 2020: Multimedia Retrieval in Lifelogging, Medical, Nature, and Internet Applications",
booktitle="Advances in Information Retrieval",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="533--541",
abstract="This paper presents an overview of the 2020 ImageCLEF lab that will be organized as part of the Conference and Labs of the Evaluation Forum---CLEF Labs 2020 in Thessaloniki, Greece. ImageCLEF is an ongoing evaluation initiative (run since 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval of visual data with the aim of providing information access to large collections of images in various usage scenarios and domains. In 2020, the 18th edition of ImageCLEF will organize four main tasks: (i) a Lifelog task (videos, images and other sources) about daily activity understanding, retrieval and summarization, (ii) a Medical task that groups three previous tasks (caption analysis, tuberculosis prediction, and medical visual question answering) with new data and adapted tasks, (iii) a Coral task about segmenting and labeling collections of coral images for 3D modeling, and a new (iv) Web user interface task addressing the problems of detecting and recognizing hand drawn website UIs (User Interfaces) for generating automatic code. The strong participation, with over 235 research groups registering and 63 submitting over 359 runs for the tasks in 2019 shows an important interest in this benchmarking campaign. We expect the new tasks to attract at least as many researchers for 2020.",
isbn="978-3-030-45442-5"
}

